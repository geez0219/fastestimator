{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011514,
     "end_time": "2020-07-09T06:26:30.953819",
     "exception": false,
     "start_time": "2020-07-09T06:26:30.942305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Progressive Growing GAN (PGGAN)\n",
    "In this notebook, we will demonstrate the functionality of ``Scheduler`` which enables advanced training schemes such as the progressive training method described in [Karras et al.](https://arxiv.org/pdf/1710.10196.pdf). \n",
    "We will train a PGGAN to produce synthetic frontal chest X-ray images where both the generator and the discriminator grow from $4\\times4$ to $128\\times128$.\n",
    "\n",
    "### Progressive Growing Strategy\n",
    "[Karras et al.](https://arxiv.org/pdf/1710.10196.pdf) propose a training scheme in which both the generator and the discriminator progressively grow from a low resolution to a high resolution.\n",
    "Both networks begin their training based on $4\\times4$ images as illustrated below.\n",
    "![4x4](./Figure/pggan_4x4.png)\n",
    "Then, both networks progress from $4\\times4$ to $8\\times8$ by an adding an additional block that contains a couple of convolutional layers.\n",
    "![8x8](./Figure/pggan_8x8.png)\n",
    "Both the generator and the discriminator progressively grow until reaching the desired resolution of $1024\\times 1024$.\n",
    "![1024x1024](./Figure/pggan_1024x1024.png)\n",
    "*Image Credit: [Presentation slide](https://drive.google.com/open?id=1jYlrX4DgTs2VAfRcyl3pcNI4ONkBg3-g)*\n",
    "\n",
    "### Smooth Transition between Resolutions\n",
    "However, when growing the networks, the new blocks must be slowly faded into the networks in order to smoothly transition between different resolutions.\n",
    "For example, when growing the generator from $16\\times16$ to $32\\times32$, the newly added block of $32\\times32$ is slowly faded into the already well trained $16\\times16$ network by linearly increasing a fade-factor $\\alpha$ from $0$ to $1$.\n",
    "Once the network is fully transitioned to $32\\times32$, the network is trained a bit further to stabilize before growing to $64\\times64$.\n",
    "![grow](./Figure/pggan_smooth_grow.png)\n",
    "*Image Credit: [PGGAN Paper](https://arxiv.org/pdf/1710.10196.pdf)*\n",
    "\n",
    "With this progressive training strategy, PGGAN has achieved the state-of-the-art results in producing high fidelity synthetic images.\n",
    "\n",
    "## Problem Setting\n",
    "In this PGGAN example, we decided the following:\n",
    "* 560K images will be used when transitioning from a lower resolution to a higher resolution.\n",
    "* 560K images will be used when stabilizing the fully transitioned network.\n",
    "* Initial resolution will be $4\\times4$.\n",
    "* Final resolution will be $128\\times128$.\n",
    "\n",
    "The number of images for both transitioning and stabilizing is equivalent to 5 epochs; the networks would smoothly grow over 5 epochs and would stabilize for 5 epochs. This yields the following schedule for growing both networks:\n",
    "\n",
    "* From $1^{st}$ epoch to $5^{th}$ epoch: train $4\\times4$ resolution\n",
    "* From $6^{th}$ epoch to $10^{th}$ epoch: transition from $4\\times4$ to $8\\times8$\n",
    "* From $11^{th}$ epoch to $15^{th}$ epoch: stabilize $8\\times8$\n",
    "* From $16^{th}$ epoch to $20^{th}$ epoch: transition from $8\\times8$ to $16\\times16$\n",
    "* From $21^{st}$ epoch to $25^{th}$ epoch: stabilize $16\\times16$\n",
    "* From $26^{th}$ epoch to $30^{th}$ epoch: transition from $16\\times16$ to $32\\times32$\n",
    "* From $31^{st}$ epoch to $35^{th}$ epoch: stabilize $32\\times32$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "* From $51^{th}$ epoch to $55^{th}$ epoch: stabilize $128\\times128$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:30.970169Z",
     "iopub.status.busy": "2020-07-09T06:26:30.969832Z",
     "iopub.status.idle": "2020-07-09T06:26:33.453233Z",
     "shell.execute_reply": "2020-07-09T06:26:33.452936Z"
    },
    "papermill": {
     "duration": 2.492884,
     "end_time": "2020-07-09T06:26:33.453303",
     "exception": false,
     "start_time": "2020-07-09T06:26:30.960419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.schedule import EpochScheduler\n",
    "from fastestimator.util import get_num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:33.472595Z",
     "iopub.status.busy": "2020-07-09T06:26:33.472212Z",
     "iopub.status.idle": "2020-07-09T06:26:33.473672Z",
     "shell.execute_reply": "2020-07-09T06:26:33.473382Z"
    },
    "papermill": {
     "duration": 0.013676,
     "end_time": "2020-07-09T06:26:33.473736",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.460060",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "target_size=128\n",
    "epochs=55\n",
    "save_dir=tempfile.mkdtemp()\n",
    "max_train_steps_per_epoch=None\n",
    "data_dir=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:33.490890Z",
     "iopub.status.busy": "2020-07-09T06:26:33.490539Z",
     "iopub.status.idle": "2020-07-09T06:26:33.492167Z",
     "shell.execute_reply": "2020-07-09T06:26:33.491881Z"
    },
    "papermill": {
     "duration": 0.011475,
     "end_time": "2020-07-09T06:26:33.492230",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.480755",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 3\n",
    "target_size = 8\n",
    "max_train_steps_per_epoch = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006763,
     "end_time": "2020-07-09T06:26:33.505911",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.499148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure growing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:33.525144Z",
     "iopub.status.busy": "2020-07-09T06:26:33.524811Z",
     "iopub.status.idle": "2020-07-09T06:26:33.526087Z",
     "shell.execute_reply": "2020-07-09T06:26:33.526357Z"
    },
    "papermill": {
     "duration": 0.013203,
     "end_time": "2020-07-09T06:26:33.526429",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.513226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_grow = np.log2(target_size) - 2\n",
    "assert num_grow >= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\"\n",
    "num_phases = int(2 * num_grow + 1)\n",
    "assert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size)\n",
    "num_grow, phase_length = int(num_grow), int(epochs / num_phases)\n",
    "event_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)]\n",
    "event_size = [4] + [2**(i + 3) for i in range(num_grow)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008337,
     "end_time": "2020-07-09T06:26:33.541688",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.533351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining Input `Pipeline`\n",
    "\n",
    "First, we need to download the chest frontal X-ray dataset from the National Institute of Health (NIH); the dataset has over 112,000 images with resolution $1024\\times1024$. We use the pre-built ``fastestimator.dataset.nih_chestxray`` API to download these images. A detailed description of the dataset is available [here](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community).\n",
    "\n",
    "### Note: Please make sure to have a stable internet connection when downloading the dataset for the first time since the size of the dataset is over 40GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:33.557981Z",
     "iopub.status.busy": "2020-07-09T06:26:33.557584Z",
     "iopub.status.idle": "2020-07-09T06:26:33.852055Z",
     "shell.execute_reply": "2020-07-09T06:26:33.851757Z"
    },
    "papermill": {
     "duration": 0.303559,
     "end_time": "2020-07-09T06:26:33.852128",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.548569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import nih_chestxray\n",
    "\n",
    "dataset = nih_chestxray.load_data(root_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007364,
     "end_time": "2020-07-09T06:26:33.866314",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.858950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Given the images, we need the following preprocessing operations to execute dynamically for every batch:\n",
    "1. Read the image.\n",
    "2. Resize the image to the correct size based on the current epoch.\n",
    "3. Create a lower resolution of the image, which is accomplished by downsampling by a factor of 2 then upsampling by a factor of 2.\n",
    "4. Rescale the pixels of both the original image and lower resolution image to the range [-1, 1]\n",
    "5. Convert both the original image and lower resolution image from channel last to channel first\n",
    "6. Create the latent vector used by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:33.886510Z",
     "iopub.status.busy": "2020-07-09T06:26:33.886165Z",
     "iopub.status.idle": "2020-07-09T06:26:33.896420Z",
     "shell.execute_reply": "2020-07-09T06:26:33.896077Z"
    },
    "papermill": {
     "duration": 0.023536,
     "end_time": "2020-07-09T06:26:33.896487",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.872951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.op import LambdaOp\n",
    "from fastestimator.op.numpyop.multivariate import Resize\n",
    "from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n",
    "\n",
    "resize_map = {\n",
    "    epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)\n",
    "    for (epoch, size) in zip(event_epoch, event_size)\n",
    "}\n",
    "resize_low_res_map1 = {\n",
    "    epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)\n",
    "    for (epoch, size) in zip(event_epoch, event_size)\n",
    "}\n",
    "resize_low_res_map2 = {\n",
    "    epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)\n",
    "    for (epoch, size) in zip(event_epoch, event_size)\n",
    "}\n",
    "batch_size_map = {\n",
    "    epoch: max(512 // size, 4) * get_num_devices() if size <= 512 else 2 * get_num_devices()\n",
    "    for (epoch, size) in zip(event_epoch, event_size)\n",
    "}\n",
    "batch_scheduler = EpochScheduler(epoch_dict=batch_size_map)\n",
    "pipeline = fe.Pipeline(\n",
    "    batch_size=batch_scheduler,\n",
    "    train_data=dataset,\n",
    "    drop_last=True,\n",
    "    ops=[\n",
    "        ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),\n",
    "        EpochScheduler(epoch_dict=resize_map),\n",
    "        EpochScheduler(epoch_dict=resize_low_res_map1),\n",
    "        EpochScheduler(epoch_dict=resize_low_res_map2),\n",
    "        Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),\n",
    "        ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),\n",
    "        LambdaOp(fn=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00822,
     "end_time": "2020-07-09T06:26:33.911721",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.903501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's visualize how our `Pipeline` changes image resolution at the different epochs we specified using `Schedulers`. FastEstimator as a ``get_results`` method to aid in this. In order to correctly visualize the output of the `Pipeline`, we need to provide epoch numbers to the `get_results` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:33.930830Z",
     "iopub.status.busy": "2020-07-09T06:26:33.930467Z",
     "iopub.status.idle": "2020-07-09T06:26:38.210528Z",
     "shell.execute_reply": "2020-07-09T06:26:38.210208Z"
    },
    "papermill": {
     "duration": 4.291922,
     "end_time": "2020-07-09T06:26:38.210592",
     "exception": false,
     "start_time": "2020-07-09T06:26:33.918670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x3600 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "for i, epoch in enumerate(event_epoch):\n",
    "    batch_data = pipeline.get_results(epoch=epoch)\n",
    "    img = np.squeeze(batch_data[\"x\"][0] + 0.5)\n",
    "    plt.subplot(1, 9, i+1)\n",
    "    plt.imshow(img, cmap='gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007735,
     "end_time": "2020-07-09T06:26:38.225893",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.218158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining `Network`\n",
    "### Defining the generator and the discriminator\n",
    "To express the progressive growing of networks, we return a list of models that progressively grow from $4 \\times 4$ to $1024 \\times 1024$ such that $i^{th}$ model in the list is a superset of the previous models. We define a ``fade_in_alpha`` to control the smoothness of growth. ``fe.build`` then bundles each model, optimizer, and model name together for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:38.253954Z",
     "iopub.status.busy": "2020-07-09T06:26:38.243230Z",
     "iopub.status.idle": "2020-07-09T06:26:38.517666Z",
     "shell.execute_reply": "2020-07-09T06:26:38.517326Z"
    },
    "papermill": {
     "duration": 0.284234,
     "end_time": "2020-07-09T06:26:38.517731",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.233497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):\n",
    "    return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)\n",
    "\n",
    "\n",
    "class EqualizedLRDense(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, gain=np.sqrt(2)):\n",
    "        super().__init__(in_features, out_features, bias=False)\n",
    "        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n",
    "        self.wscale = np.float32(gain / np.sqrt(in_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x) * self.wscale\n",
    "\n",
    "\n",
    "class ApplyBias(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(in_features))\n",
    "        torch.nn.init.constant_(self.bias.data, val=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 4:\n",
    "            x = x + self.bias.view(1, -1, 1, 1).expand_as(x)\n",
    "        else:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class EqualizedLRConv2D(torch.nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)\n",
    "        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n",
    "        fan_in = np.float32(np.prod(self.weight.data.shape[1:]))\n",
    "        self.wscale = np.float32(gain / np.sqrt(fan_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x) * self.wscale\n",
    "\n",
    "\n",
    "def pixel_normalization(x, eps=1e-8):\n",
    "    return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)\n",
    "\n",
    "\n",
    "def mini_batch_std(x, group_size=4, eps=1e-8):\n",
    "    b, c, h, w = x.shape\n",
    "    group_size = min(group_size, b)\n",
    "    y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]\n",
    "    y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]\n",
    "    y = torch.mean(y**2, axis=0)  # [M, C, H, W]\n",
    "    y = torch.sqrt(y + eps)  # [M, C, H, W]\n",
    "    y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]\n",
    "    y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]\n",
    "    return torch.cat((x, y), 1)\n",
    "\n",
    "\n",
    "def fade_in(x, y, alpha):\n",
    "    return (1.0 - alpha) * x + alpha * y\n",
    "\n",
    "\n",
    "class ToRGB(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_channels=3):\n",
    "        super().__init__()\n",
    "        self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)\n",
    "        self.bias = ApplyBias(in_features=num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elr_conv2d(x)\n",
    "        x = self.bias(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FromRGB(torch.nn.Module):\n",
    "    def __init__(self, res, num_channels=3):\n",
    "        super().__init__()\n",
    "        self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)\n",
    "        self.bias = ApplyBias(in_features=_nf(res - 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elr_conv2d(x)\n",
    "        x = self.bias(x)\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockG1D(torch.nn.Module):\n",
    "    def __init__(self, res=2, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)\n",
    "        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n",
    "        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n",
    "        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n",
    "        self.res = res\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 512]\n",
    "        x = pixel_normalization(x)  # [batch, 512]\n",
    "        x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]\n",
    "        x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n",
    "        x = pixel_normalization(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockG2D(torch.nn.Module):\n",
    "    def __init__(self, res):\n",
    "        super().__init__()\n",
    "        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))\n",
    "        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n",
    "        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n",
    "        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n",
    "        self.upsample = torch.nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]\n",
    "        x = self.upsample(x)\n",
    "        x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n",
    "        return x\n",
    "\n",
    "\n",
    "def _block_G(res, latent_dim=512, initial_resolution=2):\n",
    "    if res == initial_resolution:\n",
    "        model = BlockG1D(res=res, latent_dim=latent_dim)\n",
    "    else:\n",
    "        model = BlockG2D(res=res)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Gen(torch.nn.Module):\n",
    "    def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):\n",
    "        super().__init__()\n",
    "        self.g_blocks = torch.nn.ModuleList(g_blocks)\n",
    "        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n",
    "        self.fade_in_alpha = fade_in_alpha\n",
    "        self.upsample = torch.nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for g in self.g_blocks[:-1]:\n",
    "            x = g(x)\n",
    "        previous_img = self.rgb_blocks[0](x)\n",
    "        previous_img = self.upsample(previous_img)\n",
    "        x = self.g_blocks[-1](x)\n",
    "        new_img = self.rgb_blocks[1](x)\n",
    "        return fade_in(previous_img, new_img, self.fade_in_alpha)\n",
    "\n",
    "\n",
    "def build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):\n",
    "    g_blocks = [\n",
    "        _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)\n",
    "    ]\n",
    "    rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]\n",
    "    generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]\n",
    "    for idx in range(2, len(g_blocks) + 1):\n",
    "        generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n",
    "    final_model_list = g_blocks + [rgb_blocks[-1]]\n",
    "    generators.append(torch.nn.Sequential(*final_model_list))\n",
    "    return generators\n",
    "\n",
    "\n",
    "class BlockD1D(torch.nn.Module):\n",
    "    def __init__(self, res=2):\n",
    "        super().__init__()\n",
    "        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))\n",
    "        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n",
    "        self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))\n",
    "        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n",
    "        self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)\n",
    "        self.bias3 = ApplyBias(in_features=1)\n",
    "        self.res = res\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 512, 4, 4]\n",
    "        x = mini_batch_std(x)  # [batch, 513, 4, 4]\n",
    "        x = self.elr_conv2d(x)  # [batch, 512, 4, 4]\n",
    "        x = self.bias1(x)  # [batch, 512, 4, 4]\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]\n",
    "        x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]\n",
    "        x = self.elr_dense1(x)  # [batch, 512]\n",
    "        x = self.bias2(x)  # [batch, 512]\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]\n",
    "        x = self.elr_dense2(x)  # [batch, 1]\n",
    "        x = self.bias3(x)  # [batch, 1]\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockD2D(torch.nn.Module):\n",
    "    def __init__(self, res):\n",
    "        super().__init__()\n",
    "        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n",
    "        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n",
    "        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))\n",
    "        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n",
    "        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elr_conv2d1(x)\n",
    "        x = self.bias1(x)\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
    "        x = self.elr_conv2d2(x)\n",
    "        x = self.bias2(x)\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _block_D(res, initial_resolution=2):\n",
    "    if res == initial_resolution:\n",
    "        model = BlockD1D(res)\n",
    "    else:\n",
    "        model = BlockD2D(res)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Disc(torch.nn.Module):\n",
    "    def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):\n",
    "        super().__init__()\n",
    "        self.d_blocks = torch.nn.ModuleList(d_blocks)\n",
    "        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n",
    "        self.fade_in_alpha = fade_in_alpha\n",
    "        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_x = self.rgb_blocks[1](x)\n",
    "        new_x = self.d_blocks[-1](new_x)\n",
    "        downscale_x = self.pool(x)\n",
    "        downscale_x = self.rgb_blocks[0](downscale_x)\n",
    "        x = fade_in(downscale_x, new_x, self.fade_in_alpha)\n",
    "        for d in self.d_blocks[:-1][::-1]:\n",
    "            x = d(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):\n",
    "    d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]\n",
    "    rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]\n",
    "    discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]\n",
    "    for idx in range(2, len(d_blocks) + 1):\n",
    "        discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n",
    "    return discriminators\n",
    "\n",
    "\n",
    "\n",
    "fade_in_alpha = torch.tensor(1.0)\n",
    "d_models = fe.build(\n",
    "    model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n",
    "    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),\n",
    "    model_name=[\"d_{}\".format(size) for size in event_size])\n",
    "\n",
    "g_models = fe.build(\n",
    "    model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n",
    "    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],\n",
    "    model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007822,
     "end_time": "2020-07-09T06:26:38.533249",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.525427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Following operations will happen in our `Network`:\n",
    "1. random vector -> generator -> fake images \n",
    "2. fake images -> discriminator -> fake scores\n",
    "3. real image, low resolution real image -> blender -> blended real images\n",
    "4. blended real images -> discriminator -> real scores\n",
    "5. fake images, real images -> interpolater -> interpolated images\n",
    "6. interpolated images -> discriminator -> interpolated scores\n",
    "7. interpolated scores, interpolated image -> get_gradient -> gradient penalty \n",
    "8. fake_score -> GLoss -> generator loss\n",
    "9. real score, fake score, gradient penalty -> DLoss -> discriminator loss\n",
    "10. update generator\n",
    "11. update discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:38.572857Z",
     "iopub.status.busy": "2020-07-09T06:26:38.572485Z",
     "iopub.status.idle": "2020-07-09T06:26:38.573698Z",
     "shell.execute_reply": "2020-07-09T06:26:38.573947Z"
    },
    "papermill": {
     "duration": 0.033311,
     "end_time": "2020-07-09T06:26:38.574026",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.540715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import TensorOp\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.backend import feed_forward, get_gradient\n",
    "\n",
    "class ImageBlender(TensorOp):\n",
    "    def __init__(self, alpha, inputs=None, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        image, image_lowres = data\n",
    "        new_img = self.alpha * image + (1 - self.alpha) * image_lowres\n",
    "        return new_img\n",
    "\n",
    "\n",
    "class Interpolate(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        fake, real = data\n",
    "        batch_size = real.shape[0]\n",
    "        coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)\n",
    "        return real + (fake - real) * coeff\n",
    "\n",
    "\n",
    "class GradientPenalty(TensorOp):\n",
    "    def __init__(self, inputs, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        x_interp, interp_score = data\n",
    "        gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)\n",
    "        grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))\n",
    "        gp = (grad_l2 - 1.0)**2\n",
    "        return gp\n",
    "\n",
    "\n",
    "class GLoss(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        return -torch.mean(data)\n",
    "\n",
    "\n",
    "class DLoss(TensorOp):\n",
    "    \"\"\"Compute discriminator loss.\"\"\"\n",
    "    def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.wgan_lambda = wgan_lambda\n",
    "        self.wgan_epsilon = wgan_epsilon\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        real_score, fake_score, gp = data\n",
    "        loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "fake_img_map = {\n",
    "    epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)\n",
    "    for (epoch, model) in zip(event_epoch, g_models[:-1])\n",
    "}\n",
    "fake_score_map = {\n",
    "    epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)\n",
    "    for (epoch, model) in zip(event_epoch, d_models)\n",
    "}\n",
    "real_score_map = {\n",
    "    epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)\n",
    "    for (epoch, model) in zip(event_epoch, d_models)\n",
    "}\n",
    "interp_score_map = {\n",
    "    epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)\n",
    "    for (epoch, model) in zip(event_epoch, d_models)\n",
    "}\n",
    "g_update_map = {\n",
    "    epoch: UpdateOp(loss_name=\"gloss\", model=model)\n",
    "    for (epoch, model) in zip(event_epoch, g_models[:-1])\n",
    "}\n",
    "d_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)}\n",
    "network = fe.Network(ops=[\n",
    "    EpochScheduler(fake_img_map),\n",
    "    EpochScheduler(fake_score_map),\n",
    "    ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),\n",
    "    EpochScheduler(real_score_map),\n",
    "    Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),\n",
    "    EpochScheduler(interp_score_map),\n",
    "    GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),\n",
    "    GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n",
    "    DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),\n",
    "    EpochScheduler(g_update_map),\n",
    "    EpochScheduler(d_update_map)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007642,
     "end_time": "2020-07-09T06:26:38.589399",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.581757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining Estimator\n",
    "\n",
    "Given that ``Pipeline`` and ``Network`` are properly defined, we need to define an `AlphaController` `Trace` to help both the generator and the discriminator smoothly grow by controlling the value of the `fade_in_alpha` tensor created previously.  We will also use `ModelSaver` to save our model during every training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:38.616071Z",
     "iopub.status.busy": "2020-07-09T06:26:38.615721Z",
     "iopub.status.idle": "2020-07-09T06:26:38.617304Z",
     "shell.execute_reply": "2020-07-09T06:26:38.616990Z"
    },
    "papermill": {
     "duration": 0.020676,
     "end_time": "2020-07-09T06:26:38.617366",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.596690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.trace import Trace\n",
    "from fastestimator.trace.io import ModelSaver\n",
    "\n",
    "class AlphaController(Trace):\n",
    "    def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):\n",
    "        super().__init__(inputs=None, outputs=None, mode=\"train\")\n",
    "        self.alpha = alpha\n",
    "        self.fade_start_epochs = fade_start_epochs\n",
    "        self.duration = duration\n",
    "        self.batch_scheduler = batch_scheduler\n",
    "        self.num_examples = num_examples\n",
    "        self.change_alpha = False\n",
    "        self.nimg_total = self.duration * self.num_examples\n",
    "        self._idx = 0\n",
    "        self.nimg_so_far = 0\n",
    "        self.current_batch_size = None\n",
    "\n",
    "    def on_epoch_begin(self, state):\n",
    "        # check whetehr the current epoch is in smooth transition of resolutions\n",
    "        fade_epoch = self.fade_start_epochs[self._idx]\n",
    "        if self.system.epoch_idx == fade_epoch:\n",
    "            self.change_alpha = True\n",
    "            self.nimg_so_far = 0\n",
    "            self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)\n",
    "            print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))\n",
    "        elif self.system.epoch_idx == fade_epoch + self.duration:\n",
    "            print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))\n",
    "            self.change_alpha = False\n",
    "            if self._idx + 1 < len(self.fade_start_epochs):\n",
    "                self._idx += 1\n",
    "            self.alpha.data = torch.tensor(1.0)\n",
    "\n",
    "    def on_batch_begin(self, state):\n",
    "        # if in resolution transition, smoothly change the alpha from 0 to 1\n",
    "        if self.change_alpha:\n",
    "            self.nimg_so_far += self.current_batch_size\n",
    "            self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)\n",
    "            \n",
    "traces = [\n",
    "    AlphaController(alpha=fade_in_alpha,\n",
    "                    fade_start_epochs=event_epoch[1:],\n",
    "                    duration=phase_length,\n",
    "                    batch_scheduler=batch_scheduler,\n",
    "                    num_examples=len(dataset)),\n",
    "    ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]\n",
    "            \n",
    "            \n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces,\n",
    "                         max_train_steps_per_epoch=max_train_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008037,
     "end_time": "2020-07-09T06:26:38.632639",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.624602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start Training\n",
    "\n",
    "### Note: for 128x128 resolution, it takes about 24 hours on single V100 GPU.    for 1024x1024 resolution, it takes ~ 2.5 days on 4 V100 GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-09T06:26:38.687364Z",
     "iopub.status.busy": "2020-07-09T06:26:38.670884Z",
     "iopub.status.idle": "2020-07-09T06:28:39.701891Z",
     "shell.execute_reply": "2020-07-09T06:28:39.702160Z"
    },
    "papermill": {
     "duration": 121.062058,
     "end_time": "2020-07-09T06:28:39.702234",
     "exception": false,
     "start_time": "2020-07-09T06:26:38.640176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 1; dloss: 1.1774517; gloss: 0.37516928; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-ModelSaver: Saved model to /tmp/tmpx8toa88b/G_epoch_1.pt\n",
      "FastEstimator-Train: step: 20; epoch: 1; epoch_time: 19.71 sec; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Alpha: Started fading in for size 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-ModelSaver: Saved model to /tmp/tmpx8toa88b/G_epoch_2.pt\n",
      "FastEstimator-Train: step: 40; epoch: 2; epoch_time: 46.29 sec; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Alpha: Finished fading in for size 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-ModelSaver: Saved model to /tmp/tmpx8toa88b/G_epoch_3.pt\n",
      "FastEstimator-Train: step: 60; epoch: 3; epoch_time: 45.48 sec; \n",
      "FastEstimator-Finish: step: 60; total_time: 115.53 sec; g_8_lr: 0.001; d_4_lr: 0.001; g_4_lr: 0.001; d_8_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "duration": 131.086105,
   "end_time": "2020-07-09T06:28:41.375388",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/geez219/python_project/fastestimator/apphub/image_generation/pggan/pggan.ipynb",
   "output_path": "/home/geez219/python_project/fastestimator/test/apphub_scripts/image_generation/pggan/pggan_out.ipynb",
   "parameters": {
    "epochs": 3,
    "max_train_steps_per_epoch": 20,
    "target_size": 8
   },
   "start_time": "2020-07-09T06:26:30.289283",
   "version": "2.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}