{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Instance Detection with RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to implement [RetinaNet by Lin et al., 2017](https://arxiv.org/abs/1708.02002) for [COCO dataset](http://cocodataset.org/) instance detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import fastestimator as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#training parameters\n",
    "data_dir=None\n",
    "batch_size = 16\n",
    "epochs = 13\n",
    "image_size=512\n",
    "num_classes=90\n",
    "max_train_steps_per_epoch = None\n",
    "max_eval_steps_per_epoch = None\n",
    "model_dir=tempfile.mkdtemp()\n",
    "class_json_path = 'class.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 2\n",
    "batch_size = 8\n",
    "max_train_steps_per_epoch = 10\n",
    "max_eval_steps_per_epoch = 10\n",
    "class_json_path = \"/home/geez219/python_project/fastestimator/test/apphub_scripts/instance_detection/retinanet/class.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1 - Data and `Pipeline` preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `class.json` includes a map for the class number and what object the number corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(class_json_path, 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We call our `mscoco` data API to obtain the training and validation set:\n",
    "* 118287 images for training\n",
    "* 5000 images for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mscoco\n",
    "\n",
    "train_ds, eval_ds = mscoco.load_data(root_dir=data_dir)\n",
    "print(len(train_ds)) # 118287\n",
    "print(len(eval_ds)) # 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Anchors are predefined for each pixel in the feature map. In this apphub our backbone is ResNet-50, so we can precompute all the anchors we need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_fpn_anchor_box(width, height):\n",
    "    assert height % 32 == 0 and width % 32 == 0\n",
    "    shapes = [(int(height / 8), int(width / 8))]  # P3\n",
    "    num_pixel = [np.prod(shapes)]\n",
    "    anchor_lengths = [32, 64, 128, 256, 512]\n",
    "    for _ in range(4):  # P4 through P7\n",
    "        shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))\n",
    "        num_pixel.append(np.prod(shapes[-1]))\n",
    "    total_num_pixels = np.sum(num_pixel)\n",
    "    anchorbox = np.zeros((9 * total_num_pixels, 4))\n",
    "    anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]\n",
    "    aspect_ratios = [1.0, 2.0, 0.5]  #x:y\n",
    "    anchor_idx = 0\n",
    "    for shape, anchor_length in zip(shapes, anchor_lengths):\n",
    "        p_h, p_w = shape\n",
    "        base_y = 2**np.ceil(np.log2(height / p_h))\n",
    "        base_x = 2**np.ceil(np.log2(width / p_w))\n",
    "        for i in range(p_h):\n",
    "            center_y = (i + 1 / 2) * base_y\n",
    "            for j in range(p_w):\n",
    "                center_x = (j + 1 / 2) * base_x\n",
    "                for anchor_length_multiplier in anchor_length_multipliers:\n",
    "                    area = (anchor_length * anchor_length_multiplier)**2\n",
    "                    for aspect_ratio in aspect_ratios:\n",
    "                        x1 = center_x - np.sqrt(area * aspect_ratio) / 2\n",
    "                        y1 = center_y - np.sqrt(area / aspect_ratio) / 2\n",
    "                        x2 = center_x + np.sqrt(area * aspect_ratio) / 2\n",
    "                        y2 = center_y + np.sqrt(area / aspect_ratio) / 2\n",
    "                        anchorbox[anchor_idx, 0] = x1\n",
    "                        anchorbox[anchor_idx, 1] = y1\n",
    "                        anchorbox[anchor_idx, 2] = x2 - x1\n",
    "                        anchorbox[anchor_idx, 3] = y2 - y1\n",
    "                        anchor_idx += 1\n",
    "        if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore\n",
    "            break\n",
    "    return np.float32(anchorbox), np.int32(num_pixel) * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.op.numpyop import NumpyOp\n",
    "\n",
    "class ShiftLabel(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        # the label of COCO dataset starts from 1, shifting the start to 0\n",
    "        bbox = np.array(data, dtype=np.float32)\n",
    "        bbox[:, -1] = bbox[:, -1] - 1\n",
    "        return bbox\n",
    "\n",
    "\n",
    "class AnchorBox(NumpyOp):\n",
    "    def __init__(self, width, height, inputs, outputs, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        target = self._generate_target(data)  # bbox is #obj x 5\n",
    "        return np.float32(target)\n",
    "\n",
    "    def _generate_target(self, bbox):\n",
    "        object_boxes = bbox[:, :-1]  # num_obj x 4\n",
    "        label = bbox[:, -1]  # num_obj x 1\n",
    "        ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor\n",
    "        #now for each object in image, assign the anchor box with highest iou to them\n",
    "        anchorbox_best_iou_idx = np.argmax(ious, axis=1)\n",
    "        num_obj = ious.shape[0]\n",
    "        for row in range(num_obj):\n",
    "            ious[row, anchorbox_best_iou_idx[row]] = 0.99\n",
    "        #next, begin the anchor box assignment based on iou\n",
    "        anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1\n",
    "        anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1\n",
    "        cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1\n",
    "        cls_gt[np.where(anchor_best_iou <= 0.4)] = -1  #background class\n",
    "        cls_gt[np.where(np.logical_and(anchor_best_iou > 0.4, anchor_best_iou <= 0.5))] = -2  # ignore these examples\n",
    "        #finally, calculate localization target\n",
    "        single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4\n",
    "        gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)\n",
    "        ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)\n",
    "        dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)\n",
    "        dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)\n",
    "        dwidth = np.squeeze(np.log(gt_width / ac_width))\n",
    "        dheight = np.squeeze(np.log(gt_height / ac_height))\n",
    "        return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_iou(boxes1, boxes2):\n",
    "        \"\"\"Computes the value of intersection over union (IoU) of two array of boxes.\n",
    "        Args:\n",
    "            box1 (array): first boxes in N x 4\n",
    "            box2 (array): second box in M x 4\n",
    "        Returns:\n",
    "            float: IoU value in N x M\n",
    "        \"\"\"\n",
    "        x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)\n",
    "        x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)\n",
    "        x12 = x11 + w1\n",
    "        y12 = y11 + h1\n",
    "        x22 = x21 + w2\n",
    "        y22 = y21 + h2\n",
    "        xmin = np.maximum(x11, np.transpose(x21))\n",
    "        ymin = np.maximum(y11, np.transpose(y21))\n",
    "        xmax = np.minimum(x12, np.transpose(x22))\n",
    "        ymax = np.minimum(y12, np.transpose(y22))\n",
    "        inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)\n",
    "        area1 = (w1 + 1) * (h1 + 1)\n",
    "        area2 = (w2 + 1) * (h2 + 1)\n",
    "        iou = inter_area / (area1 + area2.T - inter_area)\n",
    "        return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For our data pipeline, we resize the images such that the longer side is 512 pixels. We keep the aspect ratio the same as original image, so on the shorter side we pad zeros. The resized image is 512 by 512. For data augmentation we only flip the image horizontally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from albumentations import BboxParams\n",
    "\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded\n",
    "from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n",
    "\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_ds,\n",
    "    eval_data=eval_ds,\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        ReadImage(inputs=\"image\", outputs=\"image\"),\n",
    "        LongestMaxSize(image_size, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params=BboxParams(\"coco\", min_area=1.0)),\n",
    "        PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\",bbox_params=BboxParams(\"coco\", min_area=1.0)),\n",
    "        Sometimes(HorizontalFlip(mode=\"train\", image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params='coco')),\n",
    "        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n",
    "        ShiftLabel(inputs=\"bbox\", outputs=\"bbox\"),\n",
    "        AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=image_size, height=image_size),\n",
    "        ChannelTranspose(inputs=\"image\", outputs=\"image\")],\n",
    "    pad_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualization of batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_data = pipeline.get_results(mode='eval', num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "step_index = 0\n",
    "batch_index = 7\n",
    "\n",
    "img = batch_data[step_index]['image'][batch_index].numpy()\n",
    "img = ((img + 1)/2 * 255).astype(np.uint8)\n",
    "img = np.transpose(img, [1, 2, 0])\n",
    "\n",
    "keep = ~np.all(batch_data[step_index]['bbox'][batch_index].numpy() == 0, axis=1)\n",
    "x1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(img)\n",
    "for j in range(len(x1)):\n",
    "    rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]+1))], color=(1, 0, 0), fontsize=14)\n",
    "\n",
    "\n",
    "print(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2 - `Network` construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Class and box subnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We define the classification (class) subnet and regression (box) subnet. See Fig. 3 of the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationSubNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, num_anchors=9):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_1.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_1.bias.data)\n",
    "        self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_2.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_2.bias.data)\n",
    "        self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_3.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_3.bias.data)\n",
    "        self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_4.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_4.bias.data)\n",
    "        self.conv2d_5 = nn.Conv2d(256, num_classes * num_anchors, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_5.weight.data, std=0.01)\n",
    "        nn.init.constant_(self.conv2d_5.bias.data, val=np.log(1 / 99))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -> [8, h, w, c] , to make reshape meaningful on position\n",
    "        return x.reshape(x.size(0), -1, self.num_classes)  # the output dimension is [batch, #anchor, #classes]\n",
    "\n",
    "\n",
    "class RegressionSubNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors=9):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_1.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_1.bias.data)\n",
    "        self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_2.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_2.bias.data)\n",
    "        self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_3.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_3.bias.data)\n",
    "        self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_4.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_4.bias.data)\n",
    "        self.conv2d_5 = nn.Conv2d(256, 4 * num_anchors, 3, padding=1)\n",
    "        nn.init.normal_(self.conv2d_5.weight.data, std=0.01)\n",
    "        nn.init.zeros_(self.conv2d_5.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2d_5(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -> [8, h, w, c] , to make reshape meaningful on position\n",
    "        return x.reshape(x.size(0), -1, 4)  # the output dimension is [batch, #anchor, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use ResNet-50 as our backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RetinaNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        res50_layers = list(torchvision.models.resnet50(pretrained=True).children())\n",
    "        self.res50_in_C3 = nn.Sequential(*(res50_layers[:6]))\n",
    "        self.res50_C3_C4 = nn.Sequential(*(res50_layers[6]))\n",
    "        self.res50_C4_C5 = nn.Sequential(*(res50_layers[7]))\n",
    "        self.conv2d_C5 = nn.Conv2d(2048, 256, 1)\n",
    "        self.conv2d_C4 = nn.Conv2d(1024, 256, 1)\n",
    "        self.conv2d_C3 = nn.Conv2d(512, 256, 1)\n",
    "        self.conv2d_P6 = nn.Conv2d(2048, 256, 3, stride=2, padding=1)\n",
    "        self.conv2d_P7 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
    "        self.conv2d_P5 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv2d_P4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv2d_P3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.cls_net = ClassificationSubNet(in_channels=256, num_classes=num_classes)\n",
    "        self.reg_net = RegressionSubNet(in_channels=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        C3 = self.res50_in_C3(x)\n",
    "        C4 = self.res50_C3_C4(C3)\n",
    "        C5 = self.res50_C4_C5(C4)\n",
    "        P5 = self.conv2d_C5(C5)\n",
    "        P5_upsampling = nn.functional.interpolate(P5, scale_factor=2)\n",
    "        P4 = self.conv2d_C4(C4)\n",
    "        P4 = P5_upsampling + P4\n",
    "        P4_upsampling = nn.functional.interpolate(P4, scale_factor=2)\n",
    "        P3 = self.conv2d_C3(C3)\n",
    "        P3 = P4_upsampling + P3\n",
    "        P6 = self.conv2d_P6(C5)\n",
    "        P7 = nn.functional.relu(P6)\n",
    "        P7 = self.conv2d_P7(P7)\n",
    "        P5 = self.conv2d_P5(P5)\n",
    "        P4 = self.conv2d_P4(P4)\n",
    "        P3 = self.conv2d_P3(P3)\n",
    "        pyramid = [P3, P4, P5, P6, P7]\n",
    "        cls_output = torch.cat([self.cls_net(x) for x in pyramid], dim=-2)\n",
    "        loc_output = torch.cat([self.reg_net(x) for x in pyramid], dim=-2)\n",
    "        return cls_output, loc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use focal loss for classification and smooth L1 for regression loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import TensorOp\n",
    "\n",
    "class RetinaLoss(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        anchorbox, cls_pred, loc_pred = data\n",
    "        batch_size = anchorbox.size(0)\n",
    "        focal_loss, l1_loss = 0.0, 0.0\n",
    "        for idx in range(batch_size):\n",
    "            single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], anchorbox[idx][:, -1].long()\n",
    "            single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]\n",
    "            single_focal_loss, anchor_obj_bool = self.focal_loss(single_cls_gt, single_cls_pred)\n",
    "            single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_bool)\n",
    "            focal_loss += single_focal_loss\n",
    "            l1_loss += single_l1_loss\n",
    "        focal_loss = focal_loss / batch_size\n",
    "        l1_loss = l1_loss / batch_size\n",
    "        total_loss = focal_loss + l1_loss\n",
    "        return total_loss, focal_loss, l1_loss\n",
    "\n",
    "    def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):\n",
    "        # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]\n",
    "        num_classes = single_cls_pred.size(-1)\n",
    "        # gather the objects and background, discard the rest\n",
    "        anchor_obj_bool = single_cls_gt >= 0\n",
    "        anchor_background_obj_bool = single_cls_gt >= -1\n",
    "        anchor_background_bool = single_cls_gt == -1\n",
    "        # create one hot encoder, make -1 (background) and -2 (ignore) encoded as 0 in ground truth\n",
    "        single_cls_gt[single_cls_gt < 0] = 0\n",
    "        single_cls_gt = nn.functional.one_hot(single_cls_gt, num_classes=num_classes)\n",
    "        single_cls_gt[anchor_background_bool] = 0\n",
    "        single_cls_gt = single_cls_gt[anchor_background_obj_bool]  # remove all ignore cases\n",
    "        single_cls_gt = single_cls_gt.view(-1)\n",
    "        single_cls_pred = single_cls_pred[anchor_background_obj_bool]\n",
    "        single_cls_pred = single_cls_pred.view(-1)\n",
    "        # compute the focal weight on each selected anchor box\n",
    "        alpha_factor = torch.ones_like(single_cls_gt) * alpha\n",
    "        alpha_factor = torch.where(single_cls_gt == 1, alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = torch.where(single_cls_gt == 1, 1 - single_cls_pred, single_cls_pred)\n",
    "        focal_weight = alpha_factor * focal_weight**gamma / torch.sum(anchor_obj_bool)\n",
    "        focal_loss = nn.functional.binary_cross_entropy(input=single_cls_pred,\n",
    "                                                        target=single_cls_gt.float(),\n",
    "                                                        weight=focal_weight.detach(),\n",
    "                                                        reduction=\"sum\")\n",
    "        return focal_loss, anchor_obj_bool\n",
    "\n",
    "    def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_bool, beta=0.1):\n",
    "        # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]\n",
    "        single_loc_pred = single_loc_pred[anchor_obj_bool]  # anchor_obj_count x 4\n",
    "        single_loc_gt = single_loc_gt[anchor_obj_bool]  # anchor_obj_count x 4\n",
    "        single_loc_pred = single_loc_pred.view(-1)\n",
    "        single_loc_gt = single_loc_gt.view(-1)\n",
    "        loc_diff = torch.abs(single_loc_gt - single_loc_pred)\n",
    "        loc_loss = torch.where(loc_diff < beta, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n",
    "        loc_loss = torch.sum(loc_loss) / torch.sum(anchor_obj_bool)\n",
    "        return loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The learning rate has a warm up phase when step number is < 1000. After that, we reduce the learning rate by 10 at 60k and 80k respectively. The original batch size in the paper is 16 for 8 GPUs. Here we are using 1GPU, with batch size 16 due to a smaller image size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_fn(step):\n",
    "    if step < 1000:\n",
    "        lr = (0.01 - 0.0002) / 1000 * step + 0.0002\n",
    "    elif step < 60000:\n",
    "        lr = 0.01\n",
    "    elif step < 80000:\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        lr = 0.0001\n",
    "    return lr\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: RetinaNet(num_classes=num_classes), optimizer_fn=lambda x: torch.optim.SGD(x, lr=2e-4, momentum=0.9, weight_decay=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predict Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "During evaluation, testing and inferencing, some additional postprocessing stes are required, for example, filter out lower scores and perform Non-maximal suppression on bounding box predictions. These postprocessing steps are implemented as `TensorOp` named `PredictBox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PredictBox(TensorOp):\n",
    "    \"\"\"Convert network output to bounding boxes.\n",
    "        \"\"\"\n",
    "    def __init__(self,\n",
    "                 inputs=None,\n",
    "                 outputs=None,\n",
    "                 mode=None,\n",
    "                 input_shape=(512, 512, 3),\n",
    "                 select_top_k=1000,\n",
    "                 nms_max_outputs=100,\n",
    "                 score_threshold=0.05):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.input_shape = input_shape\n",
    "        self.select_top_k = select_top_k\n",
    "        self.nms_max_outputs = nms_max_outputs\n",
    "        self.score_threshold = score_threshold\n",
    "        self.all_anchors, self.num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])\n",
    "        self.all_anchors = torch.Tensor(self.all_anchors)\n",
    "        if torch.cuda.is_available():\n",
    "            self.all_anchors = self.all_anchors.to(\"cuda\")\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        cls_pred, loc_pred = data  # [Batch, #anchor, #num_classes], [Batch, #anchor, 4]\n",
    "        batch_size = cls_pred.size(0)\n",
    "        scores_pred, labels_pred = torch.max(cls_pred, dim=-1)\n",
    "        # loc_pred -> loc_abs\n",
    "        x1_abs = loc_pred[..., 0] * self.all_anchors[..., 2] + self.all_anchors[..., 0]\n",
    "        y1_abs = loc_pred[..., 1] * self.all_anchors[..., 3] + self.all_anchors[..., 1]\n",
    "        w_abs = torch.exp(loc_pred[..., 2]) * self.all_anchors[..., 2]\n",
    "        h_abs = torch.exp(loc_pred[..., 3]) * self.all_anchors[..., 3]\n",
    "        x2_abs, y2_abs = x1_abs + w_abs, y1_abs + h_abs\n",
    "        # iterate over images\n",
    "        final_results = []\n",
    "        for idx in range(batch_size):\n",
    "            scores_pred_single = scores_pred[idx]\n",
    "            boxes_pred_single = torch.stack([x1_abs[idx], y1_abs[idx], x2_abs[idx], y2_abs[idx]], dim=-1)\n",
    "            # iterate over each pyramid to select top 1000 anchor boxes\n",
    "            start = 0\n",
    "            top_idx = []\n",
    "            for num_anchors_fpn_level in self.num_anchors_per_level:\n",
    "                fpn_scores = scores_pred_single[start:start + num_anchors_fpn_level]\n",
    "                _, selected_index = torch.topk(fpn_scores, min(self.select_top_k, int(num_anchors_fpn_level)))\n",
    "                top_idx.append(selected_index + start)\n",
    "                start += num_anchors_fpn_level\n",
    "            top_idx = torch.cat([x.long() for x in top_idx])\n",
    "            # perform nms\n",
    "            nms_keep = torchvision.ops.nms(boxes_pred_single[top_idx], scores_pred_single[top_idx], iou_threshold=0.5)\n",
    "            nms_keep = nms_keep[:self.nms_max_outputs]  # select the top nms outputs\n",
    "            top_idx = top_idx[nms_keep]  # narrow the keep index\n",
    "            results_single = [\n",
    "                x1_abs[idx][top_idx],\n",
    "                y1_abs[idx][top_idx],\n",
    "                w_abs[idx][top_idx],\n",
    "                h_abs[idx][top_idx],\n",
    "                labels_pred[idx][top_idx].float(),\n",
    "                scores_pred[idx][top_idx],\n",
    "                torch.ones_like(x1_abs[idx][top_idx])\n",
    "            ]\n",
    "            # clip bounding boxes to image size\n",
    "            results_single[0] = torch.clamp(results_single[0], min=0, max=self.input_shape[1])\n",
    "            results_single[1] = torch.clamp(results_single[1], min=0, max=self.input_shape[0])\n",
    "            results_single[2] = torch.clamp(results_single[2], min=0)\n",
    "            results_single[2] = torch.where(results_single[2] > self.input_shape[1] - results_single[0],\n",
    "                                            self.input_shape[1] - results_single[0],\n",
    "                                            results_single[2])\n",
    "            results_single[3] = torch.clamp(results_single[3], min=0)\n",
    "            results_single[3] = torch.where(results_single[3] > self.input_shape[0] - results_single[1],\n",
    "                                            self.input_shape[0] - results_single[1],\n",
    "                                            results_single[3])\n",
    "            # mark the select as 0 for any anchorbox with score lower than threshold\n",
    "            results_single[-1] = torch.where(results_single[-2] > self.score_threshold,\n",
    "                                             results_single[-1],\n",
    "                                             torch.zeros_like(results_single[-1]))\n",
    "            final_results.append(torch.stack(results_single, dim=-1))\n",
    "        return torch.stack(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n",
    "    RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n",
    "    UpdateOp(model=model, loss_name=\"total_loss\"),\n",
    "    PredictBox(input_shape=(image_size, image_size, 3), inputs=[\"cls_pred\", \"loc_pred\"], outputs=\"pred\", mode=\"!train\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3 - `Estimator` definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastestimator.trace.adapt import LRScheduler\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric import MeanAveragePrecision\n",
    "\n",
    "traces = [\n",
    "    LRScheduler(model=model, lr_fn=lr_fn),\n",
    "    BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\"),\n",
    "    MeanAveragePrecision(num_classes=num_classes, true_key='bbox', pred_key='pred', mode=\"eval\")\n",
    "]\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces,\n",
    "                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n",
    "                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n",
    "                         monitor_names=[\"l1_loss\", \"focal_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start Training\n",
    "The training will take ~14 hours on single V100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We select 4 images and visualize the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "batch_idx = 1\n",
    "sample_data = pipeline.get_results(mode=\"eval\", num_steps=batch_idx+1)[batch_idx]\n",
    "network_out = network.transform(data=sample_data, mode=\"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num_images, 2, figsize=(20, 30))\n",
    "for batch_index in range(num_images):\n",
    "    img = network_out['image'].numpy()[batch_index, ...]\n",
    "    img = ((img + 1) / 2 * 255).astype(np.uint8)\n",
    "    img = np.transpose(img, [1, 2, 0])\n",
    "\n",
    "    img2 = img.copy()\n",
    "    keep = ~np.all(network_out['bbox'][batch_index].numpy() == 0, axis=1)\n",
    "    gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T\n",
    "\n",
    "    scores = network_out['pred'][batch_index].numpy()[..., -2]\n",
    "    labels = network_out['pred'][batch_index].numpy()[..., -3]\n",
    "    keep = scores > 0.5\n",
    "    x1, y1, w, h, label, _, _ = network_out['pred'][batch_index].numpy()[keep].T\n",
    "\n",
    "    for i in range(len(gt_x1)):\n",
    "        rect = patches.Rectangle((gt_x1[i], gt_y1[i]),gt_w[i],gt_h[i],linewidth=1,edgecolor='r',facecolor='none')\n",
    "        ax[batch_index, 0].add_patch(rect)\n",
    "        ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')\n",
    "        ax[batch_index, 0].text(gt_x1[i] + 3,\n",
    "                   gt_y1[i] + 12,\n",
    "                   class_map[str(int(gt_label[i])+1)],\n",
    "                   color=(1, 0, 0),\n",
    "                   fontsize=14,\n",
    "                   fontweight='bold')\n",
    "    for j in range(len(x1)):\n",
    "        rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='b',facecolor='none')\n",
    "        ax[batch_index, 1].add_patch(rect)\n",
    "        ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')\n",
    "        ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]) + 1)], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax[batch_index, 0].imshow(img)\n",
    "    ax[batch_index, 1].imshow(img2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "duration": 0.135819,
   "end_time": "2020-07-09T14:38:08.309529",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/geez219/python_project/fastestimator/apphub/instance_detection/retinanet/retinanet.ipynb",
   "output_path": "/home/geez219/python_project/fastestimator/test/apphub_scripts/instance_detection/retinanet/retinanet_out.ipynb",
   "parameters": {
    "batch_size": 8,
    "class_json_path": "/home/geez219/python_project/fastestimator/test/apphub_scripts/instance_detection/retinanet/class.json",
    "epochs": 2,
    "max_eval_steps_per_epoch": 10,
    "max_train_steps_per_epoch": 10
   },
   "start_time": "2020-07-09T14:38:08.173710",
   "version": "2.1.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}